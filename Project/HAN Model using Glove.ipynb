{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HAN Model using Glove.ipynb","provenance":[],"authorship_tag":"ABX9TyOGTA0vBWhUKO5D8BmKYAOP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"rZOZsJ4C6Y0Z","colab_type":"code","outputId":"0dfda45f-ffec-41a8-d275-527e1997f660","executionInfo":{"status":"ok","timestamp":1589486958561,"user_tz":-330,"elapsed":37856,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XYW1_CY96im7","colab_type":"code","outputId":"21f34ddf-6c3c-4fa7-b17b-0b876a6d915f","executionInfo":{"status":"ok","timestamp":1589486958573,"user_tz":-330,"elapsed":4378,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My Drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iTVHO8OS8imF","colab_type":"code","outputId":"ca23f657-7000-46cf-db59-02b1161d5931","executionInfo":{"status":"ok","timestamp":1589486965172,"user_tz":-330,"elapsed":8792,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["ls\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":[" 394499700164290_signed.pdf\n"," C8_Chethan.vcf\n","\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/\n","'Getting started.pdf'\n"," glove.6B.100d.txt\n"," glove.6B.200d.txt\n"," glove.6B.300d.txt\n"," glove.6B.50d.txt\n"," glove.6B.zip\n","'gods Resume.pdf'\n"," Hotel_Reviews-515k-hotel-reviews-data-in-europe.csv\n"," Hotel_Reviews.csv\n"," model.h5\n","'open?id=1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp'\n","'open?id=1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp.1'\n","'open?id=1xzt06g_xeP1DitFXhJprF5VTwzNEUYhq'\n","'open?id=1xzt06g_xeP1DitFXhJprF5VTwzNEUYhq.1'\n","'open?id=1xzt06g_xeP1DitFXhJprF5VTwzNEUYhq.2'\n","'open?id=1xzt06g_xeP1DitFXhJprF5VTwzNEUYhq.3'\n"," Resume12.gdoc\n"," Resume12.pdf\n","'Resume (1).gdoc'\n","'Resume (2).gdoc'\n","'Resume (3).gdoc'\n"," Resume.gdoc\n"," wiki-news-300d-1M.vec\n"," wiki-news-300d-1M.vec.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ez4AIgQR8lMm","colab_type":"code","outputId":"0f6a4e60-4851-4d81-aa95-396be3af274c","executionInfo":{"status":"ok","timestamp":1589486967121,"user_tz":-330,"elapsed":6555,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","from gensim.models import KeyedVectors"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"h2EKzR0w8ssx","colab_type":"code","outputId":"22a76270-490f-41bf-da5b-0d4c50acbe1d","executionInfo":{"status":"ok","timestamp":1589486973726,"user_tz":-330,"elapsed":9992,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["reviews_df = pd.read_csv('Hotel_Reviews.csv')\n","print(reviews_df.dtypes)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Hotel_Address                                  object\n","Additional_Number_of_Scoring                    int64\n","Review_Date                                    object\n","Average_Score                                 float64\n","Hotel_Name                                     object\n","Reviewer_Nationality                           object\n","Negative_Review                                object\n","Review_Total_Negative_Word_Counts               int64\n","Total_Number_of_Reviews                         int64\n","Positive_Review                                object\n","Review_Total_Positive_Word_Counts               int64\n","Total_Number_of_Reviews_Reviewer_Has_Given      int64\n","Reviewer_Score                                float64\n","Tags                                           object\n","days_since_review                              object\n","lat                                           float64\n","lng                                           float64\n","dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OryFu7cl82d-","colab_type":"code","colab":{}},"source":["def clean(text):\n","    '''\n","    '''\n","    text = text.lower()\n","    text = text.replace(\"ain't\", \"am not\")\n","    text = text.replace(\"aren't\", \"are not\")\n","    text = text.replace(\"can't\", \"cannot\")\n","    text = text.replace(\"can't've\", \"cannot have\")\n","    text = text.replace(\"'cause\", \"because\")\n","    text = text.replace(\"could've\", \"could have\")\n","    text = text.replace(\"couldn't\", \"could not\")\n","    text = text.replace(\"couldn't've\", \"could not have\")\n","    text = text.replace(\"should've\", \"should have\")\n","    text = text.replace(\"should't\", \"should not\")\n","    text = text.replace(\"should't've\", \"should not have\")\n","    text = text.replace(\"would've\", \"would have\")\n","    text = text.replace(\"would't\", \"would not\")\n","    text = text.replace(\"would't've\", \"would not have\")\n","    text = text.replace(\"didn't\", \"did not\")\n","    text = text.replace(\"doesn't\", \"does not\")\n","    text = text.replace(\"don't\", \"do not\")\n","    text = text.replace(\"hadn't\", \"had not\")\n","    text = text.replace(\"hadn't've\", \"had not have\")\n","    text = text.replace(\"hasn't\", \"has not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"he'd\", \"he would\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"he'd've\", \"he would have\")\n","    text = text.replace(\"'s\", \"\")\n","    text = text.replace(\"'t\", \"\")\n","    text = text.replace(\"'ve\", \"\")\n","    text = text.replace(\".\", \" . \")\n","    text = text.replace(\"!\", \" ! \")\n","    text = text.replace(\"?\", \" ? \")\n","    text = text.replace(\";\", \" ; \")\n","    text = text.replace(\":\", \" : \")\n","    text = text.replace(\",\", \" , \")\n","    text = text.replace(\"´\", \"\")\n","    text = text.replace(\"‘\", \"\")\n","    text = text.replace(\"’\", \"\")\n","    text = text.replace(\"“\", \"\")\n","    text = text.replace(\"”\", \"\")\n","    text = text.replace(\"\\'\", \"\")\n","    text = text.replace(\"\\\"\", \"\")\n","    text = text.replace(\"-\", \"\")\n","    text = text.replace(\"–\", \"\")\n","    text = text.replace(\"—\", \"\")\n","    text = text.replace(\"[\", \"\")\n","    text = text.replace(\"]\",\"\")\n","    text = text.replace(\"{\",\"\")\n","    text = text.replace(\"}\", \"\")\n","    text = text.replace(\"/\", \"\")\n","    text = text.replace(\"|\", \"\")\n","    text = text.replace(\"(\", \"\")\n","    text = text.replace(\")\", \"\")\n","    text = text.replace(\"$\", \"\")\n","    text = text.replace(\"+\", \"\")\n","    text = text.replace(\"*\", \"\")\n","    text = text.replace(\"%\", \"\")\n","    text = text.replace(\"#\", \"\")\n","    text = text.replace(\"\\n\", \" \\n \")\n","    text = text.replace(\"\\n\", \"\")\n","    text = text.replace(\"_\", \" _ \")\n","    text = text.replace(\"_\", \"\")\n","    text = ''.join([i for i in text if not i.isdigit()])\n","\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H32mfQEm84bE","colab_type":"code","colab":{}},"source":["positive_reviews = reviews_df['Positive_Review'].values\n","negative_reviews = reviews_df['Negative_Review'].values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWPDqpwq88a8","colab_type":"code","colab":{}},"source":["cleaned_positive_reviews = [clean(r) for r in positive_reviews] \n","cleaned_negative_reviews = [clean(r) for r in negative_reviews] "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jK6n6glG886j","colab_type":"code","colab":{}},"source":["reviews_df['Positive_Review'] = cleaned_positive_reviews\n","reviews_df['Negative_Review'] = cleaned_negative_reviews"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AyRWkrc8-0-","colab_type":"code","colab":{}},"source":["# Shuffling data\n","reviews_df = reviews_df.sample(frac=1).reset_index(drop=True)\n","\n","# Extracting all text\n","positive_reviews = reviews_df['Positive_Review'].values\n","negative_reviews = reviews_df['Negative_Review'].values\n","reviews_text = []\n","\n","for p,n in zip(positive_reviews, negative_reviews) : \n","    if p in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n","        reviews_text.append(n)\n","    elif n in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n","        reviews_text.append(p)\n","    else : \n","        reviews_text.append(n)\n","        reviews_text.append(p)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8w7mNLXu9CCz","colab_type":"code","colab":{}},"source":["training_df = reviews_df.loc[:10000]\n","positive_reviews_filtered = training_df['Positive_Review'].values\n","negative_reviews_filtered = training_df['Negative_Review'].values\n","training_reviews = []\n","labels = []\n","\n","for idx,(p,n) in enumerate(zip(positive_reviews_filtered, negative_reviews_filtered)) : \n","    if p in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n","        training_reviews.append(n)\n","        labels.append(0)\n","    elif n in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] :\n","        training_reviews.append(p)\n","        labels.append(1)\n","    else :\n","        training_reviews.append(n)\n","        labels.append(0)\n","        training_reviews.append(p)\n","        labels.append(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEXlbxGm9EhZ","colab_type":"code","colab":{}},"source":["dict1 ={\n","    'reviews' : training_reviews,\n","    'labels' : labels\n","}\n","sentiment_df = pd.DataFrame.from_dict(dict1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JKKNcqUr9GrT","colab_type":"code","colab":{}},"source":["dict2 ={\n","    'reviews_text' : reviews_text\n","}\n","reviews_text_df = pd.DataFrame.from_dict(dict2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ePe6mpzn9H8f","colab_type":"code","outputId":"3feda612-97cf-485a-e0bf-a35a9a685ca2","executionInfo":{"status":"ok","timestamp":1589491005172,"user_tz":-330,"elapsed":1719,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["training_reviews[:10]"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[' great service plush rooms and a very comfortable bed ',\n"," ' the staff was extremely accommodating my travel plans changed and i arrived two days early the web site said it was fully booked but when i contacted them directly a room was available from that moment on the service was just top notch my first room was extremely comfortable in the old english manor style and i can t say enough about the beds super comfortable my second room was the teresa suite omg was it fantastic i felt like the lord of the manor i could not have been happier with my choice of hotel close to everything ',\n"," ' room very small very overpriced had to get shower gel shampoo from reception desk each day',\n"," ' good location',\n"," ' breakfast is  euro which is very expensive on the last day the air conditioning didn t work properly because apparently the outside temperature the previous day was not high enough would expect better from this hotel ',\n"," ' comfortable room with good quality bedding location is very close to ubahn station so it is quick and easy to get into central vienna ',\n"," ' parking outside getting a voucher for a car park m away  euro per day walking back very busy reception area outdated rooms and bathrooms tables way too close together in the breakfast room ',\n"," ' convenient location for duomo and transport links ',\n"," ' rooms were a bit small for the value of money but they had everything you could need and were clean and comfortable ',\n"," ' fabulous facilities and very helpful staff very cloe to the city centre and edgeware road tube station is just a few steps from the hotel ']"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"pJlb5qQs9KKB","colab_type":"code","outputId":"37ebad53-c112-46ba-ee3a-347704a6cf3e","executionInfo":{"status":"ok","timestamp":1589491390181,"user_tz":-330,"elapsed":2318,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["tokenizer = Tokenizer(num_words=200)\n","tokenizer.fit_on_texts(training_reviews)\n","sequences = tokenizer.texts_to_sequences(training_reviews)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","MAX_SEQUENCE_LENGTH = 1000\n","\n","data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"," \n","\n","labels = to_categorical(np.asarray(labels))\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)"],"execution_count":100,"outputs":[{"output_type":"stream","text":["Found 10572 unique tokens.\n","Shape of data tensor: (16817, 1000)\n","Shape of label tensor: (16817, 2, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tcwmEAAqBEq7","colab_type":"code","colab":{}},"source":["MAX_SENT_LENGTH = 50\n","MAX_SENTS = 20\n","MAX_WORDS = 30000\n","VALIDATION_SPLIT = 0.3\n","EMBEDDING_DIM = 300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7R6GzIlcE4Y7","colab_type":"code","colab":{}},"source":["class AttentionLayer(Layer):\n","    def __init__(self, attention_dim):\n","        self.init = initializers.get('normal')\n","        self.supports_masking = True\n","        self.attention_dim = attention_dim\n","        super(AttentionLayer, self).__init__()\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 1\n","        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n","        self.b = K.variable(self.init((self.attention_dim, )))\n","        self.u = K.variable(self.init((self.attention_dim, 1)))\n","        self.trainable_weights = [self.W, self.b, self.u]\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        ##return mask\n","        return None\n","\n","    def call(self, x, mask=None):\n","        # size of x :[batch_size, sel_len, attention_dim]\n","        # size of u :[batch_size, attention_dim]\n","        # uit = tanh(xW+b)\n","        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n","        ait = K.dot(uit, self.u)\n","        ait = K.squeeze(ait, -1)\n","\n","        ait = K.exp(ait)\n","\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            ait *= K.cast(mask, K.floatx())\n","        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        ait = K.expand_dims(ait)\n","        weighted_input = x * ait\n","        output = K.sum(weighted_input, axis=1)\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXn-5vanFD6w","colab_type":"code","colab":{}},"source":["def HAN_Model():\n","    embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SENT_LENGTH,trainable=False,mask_zero=True)\n","    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","    embedded_sequences = embedding_layer(sentence_input)\n","    l_lstm = Bidirectional(GRU(20, return_sequences=True))(embedded_sequences)\n","    l_att = AttentionLayer(20)\n","    l_att=l_att(l_lstm)\n","    sentEncoder = Model(sentence_input, l_att)\n","\n","    review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","    review_encoder = TimeDistributed(sentEncoder)(review_input)\n","    l_lstm_sent = Bidirectional(GRU(20, return_sequences=True))(review_encoder)\n","    l_att_sent = AttentionLayer(22)(l_lstm_sent)\n","    preds = Dense(2, activation='softmax')(l_att_sent)\n","    model = Model(review_input, preds)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sYXn6AbXFEul","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"f3483d04-d859-4c73-b428-624ef51c4d4d","executionInfo":{"status":"error","timestamp":1589491619873,"user_tz":-330,"elapsed":918,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["model_han=HAN_Model()\n","model_han.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","print(model_han.summary())"],"execution_count":106,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-106-da0bf23a7188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_han\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHAN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-105-9f2ebda8a06c>\u001b[0m in \u001b[0;36mHAN_Model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msentence_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0ml_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ml_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36meager_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1124\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                                  \u001b[0;34m' not compatible with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m                                  'provided weight shape ' + str(w.shape))\n\u001b[0m\u001b[1;32m   1127\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer weight shape (10573, 300) not compatible with provided weight shape (10723, 100)"]}]},{"cell_type":"code","metadata":{"id":"C2M4K4YZFIQl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuDe-xDcBUU6","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(num_words=MAX_WORDS)\n","tokenizer.fit_on_texts(texts)\n","\n","data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"15zbw4PCBbaj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"8bde5e10-d61e-4624-cc59-0406163151a0","executionInfo":{"status":"error","timestamp":1589490681466,"user_tz":-330,"elapsed":1630,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["for i, sentences in enumerate(reviews):\n","    for j, sentence in enumerate(sentences):\n","        if j < MAX_SENTS:\n","            wordTokens = text_to_word_sequence(sentence)\n","            k = 0\n","            for _, word in enumerate(wordTokens):\n","                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_WORDS:\n","                    data[i, j, k] = tokenizer.word_index[word]\n","                    k = k + 1\n","\n","word_index = tokenizer.word_index\n","print('Total %s unique word tokens.' % len(word_index))\n","\n","labels = to_categorical(np.asarray(labels))\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)"],"execution_count":65,"outputs":[{"output_type":"stream","text":["Total 0 unique word tokens.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-7e3d72a51bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total %s unique word tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of data tensor:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of label tensor:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n\u001b[1;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2668\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"]}]},{"cell_type":"code","metadata":{"id":"714yFzusBv-J","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFJtTc-C9REB","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n","x_test, x_val, y_test, y_val = train_test_split(data, labels, test_size=0.5, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbBSs3CC9VV6","colab_type":"code","outputId":"225941fe-4c8a-4104-cdb4-39aa4aa97c72","executionInfo":{"status":"ok","timestamp":1589487166358,"user_tz":-330,"elapsed":166158,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["!wget https://drive.google.com/open?id=1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp\n","!unzip glove.6B.zip"],"execution_count":17,"outputs":[{"output_type":"stream","text":["--2020-05-14 20:10:03--  https://drive.google.com/open?id=1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp\n","Resolving drive.google.com (drive.google.com)... 64.233.189.102, 64.233.189.139, 64.233.189.138, ...\n","Connecting to drive.google.com (drive.google.com)|64.233.189.102|:443... connected.\n","HTTP request sent, awaiting response... 307 Temporary Redirect\n","Location: https://drive.google.com/file/d/1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp/view?usp=drive_open [following]\n","--2020-05-14 20:10:03--  https://drive.google.com/file/d/1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp/view?usp=drive_open\n","Reusing existing connection to drive.google.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘open?id=1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp.2’\n","\n","open?id=1en5jXle_cW     [ <=>                ]  68.32K  --.-KB/s    in 0.04s   \n","\n","2020-05-14 20:10:04 (1.68 MB/s) - ‘open?id=1en5jXle_cW1XTksKKoP3RBa9Nki4J9xp.2’ saved [69961]\n","\n","Archive:  glove.6B.zip\n","replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       A\n","\n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rtCTUQPJ9p1g","colab_type":"code","outputId":"85666997-76f4-4c9f-80ac-83c2b6eabc6e","executionInfo":{"status":"ok","timestamp":1589487182340,"user_tz":-330,"elapsed":15922,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Load word vectors from pre-trained dataset\n","embeddings_index = {}\n","f = open(os.path.join(os.getcwd(), 'glove.6B.100d.txt'), encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","# Embedding\n","\n","EMBED_SIZE = 100\n","\n","min_wordCount = 2\n","absent_words = 0\n","small_words = 0\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n","word_counts = tokenizer.word_counts\n","for word, i in word_index.items():\n","    if word_counts[word] > min_wordCount:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            absent_words += 1\n","    else:\n","        small_words += 1\n","print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n","      '% of total words')\n","print('Words with '+str(min_wordCount)+' or less mentions', small_words, 'which is', \"%0.2f\" % (small_words * 100 / len(word_index)),\n","      '% of total words')\n","print(str(len(word_index)-small_words-absent_words) + ' words to proceed.')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Found 400000 word vectors.\n","Total absent words are 48 which is 0.45 % of total words\n","Words with 2 or less mentions 6439 which is 60.05 % of total words\n","4235 words to proceed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UePAxvdO-JiK","colab_type":"code","outputId":"c4d12b2e-e5fa-4066-87cb-62c5408aa165","executionInfo":{"status":"ok","timestamp":1589487182342,"user_tz":-330,"elapsed":15890,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["print(\n","embedding_matrix[word_index['great']])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558\n"," -0.41846001 -0.58437002 -0.77354997 -0.87866002 -0.37858    -0.18516\n"," -0.12800001 -0.20584001 -0.22925    -0.42598999  0.3725      0.26076999\n"," -1.07019997  0.62915999 -0.091469    0.70348001 -0.4973     -0.77691001\n","  0.66044998  0.09465    -0.44893     0.018917    0.33146    -0.35021999\n"," -0.35789001  0.030313    0.22253001 -0.23236001 -0.19719    -0.0053125\n"," -0.25848001  0.58081001 -0.10705    -0.17845    -0.16205999  0.087086\n","  0.63028997 -0.76648998  0.51618999  0.14072999  1.01900005 -0.43136001\n","  0.46138    -0.43584999 -0.47567999  0.19226     0.36065     0.78987002\n","  0.088945   -2.78139997 -0.15366     0.01015     1.17980003  0.15167999\n"," -0.050112    1.26259995 -0.77526999  0.36030999  0.95761001 -0.11385\n","  0.28035    -0.02591     0.31246001 -0.15424     0.37779999 -0.13598999\n","  0.29460001 -0.31579     0.42943001  0.086969    0.019169   -0.27241999\n"," -0.31696001  0.37327     0.61997002  0.13889     0.17188001  0.30362999\n"," -1.27760005  0.044423   -0.52736002 -0.88536    -0.19428    -0.61947\n"," -0.10146    -0.26301    -0.061707    0.36627001 -0.95222998 -0.39346001\n"," -0.69182998 -1.04260004  0.28854999  0.63055998]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XJYAlGXr0bLl","colab_type":"code","colab":{}},"source":["import keras\n","from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.merge import concatenate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKctPn6rTiY4","colab_type":"code","colab":{}},"source":["!pip install gast>=0.3.2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7hjF8TMhoUY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlUttq0_T0ny","colab_type":"code","outputId":"182e137e-3c9a-4f45-b0e9-b512255e7b3b","executionInfo":{"status":"ok","timestamp":1589487667469,"user_tz":-330,"elapsed":50502,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install Tensorflow==2.0"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Collecting Tensorflow==2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 50kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (0.2.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.0.8)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.18.4)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (3.10.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (0.9.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.12.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.28.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (0.34.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.12.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (0.8.1)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 47.6MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (3.2.1)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 44.7MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Tensorflow==2.0) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->Tensorflow==2.0) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->Tensorflow==2.0) (46.1.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (3.2.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (0.4.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (1.7.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (0.2.8)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (4.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (3.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (2020.4.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->Tensorflow==2.0) (0.4.8)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=9701e29ae399934acc0a1ebe5aab0a6c79514744c7b932bd1dc9945198552db3\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: gast, tensorflow-estimator, tensorboard, Tensorflow\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","  Found existing installation: tensorboard 2.2.1\n","    Uninstalling tensorboard-2.2.1:\n","      Successfully uninstalled tensorboard-2.2.1\n","  Found existing installation: tensorflow 2.2.0\n","    Uninstalling tensorflow-2.2.0:\n","      Successfully uninstalled tensorflow-2.2.0\n","Successfully installed Tensorflow-2.0.0 gast-0.2.2 tensorboard-2.0.2 tensorflow-estimator-2.0.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"d5zQNjg-3Cmo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","#import tensorflow.contrib.keras as keras\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","import pickle\n","import itertools\n","import gensim\n","from sklearn.model_selection import train_test_split\n","from numpy import zeros\n","from sklearn.metrics import classification_report, confusion_matrix\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","from gensim.models import Word2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gc1G3QSd3DW8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"de6dd3d6-8c53-4f37-99c8-577acc721b6c","executionInfo":{"status":"ok","timestamp":1589487932443,"user_tz":-330,"elapsed":2253,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["cd /content/drive/My Drive/"],"execution_count":40,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J0Xdh_9y3SAW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"9e5a6eb0-02aa-43f0-ebe2-dafffa910679","executionInfo":{"status":"ok","timestamp":1589487944990,"user_tz":-330,"elapsed":6844,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["# Importing dataset\n","reviews_df = pd.read_csv('Hotel_Reviews.csv')\n","print(reviews_df.dtypes)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Hotel_Address                                  object\n","Additional_Number_of_Scoring                    int64\n","Review_Date                                    object\n","Average_Score                                 float64\n","Hotel_Name                                     object\n","Reviewer_Nationality                           object\n","Negative_Review                                object\n","Review_Total_Negative_Word_Counts               int64\n","Total_Number_of_Reviews                         int64\n","Positive_Review                                object\n","Review_Total_Positive_Word_Counts               int64\n","Total_Number_of_Reviews_Reviewer_Has_Given      int64\n","Reviewer_Score                                float64\n","Tags                                           object\n","days_since_review                              object\n","lat                                           float64\n","lng                                           float64\n","dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tSUuFIW53Snx","colab_type":"code","colab":{}},"source":["def clean(text):\n","    '''\n","    '''\n","    text = text.lower()\n","    text = text.replace(\"ain't\", \"am not\")\n","    text = text.replace(\"aren't\", \"are not\")\n","    text = text.replace(\"can't\", \"cannot\")\n","    text = text.replace(\"can't've\", \"cannot have\")\n","    text = text.replace(\"'cause\", \"because\")\n","    text = text.replace(\"could've\", \"could have\")\n","    text = text.replace(\"couldn't\", \"could not\")\n","    text = text.replace(\"couldn't've\", \"could not have\")\n","    text = text.replace(\"should've\", \"should have\")\n","    text = text.replace(\"should't\", \"should not\")\n","    text = text.replace(\"should't've\", \"should not have\")\n","    text = text.replace(\"would've\", \"would have\")\n","    text = text.replace(\"would't\", \"would not\")\n","    text = text.replace(\"would't've\", \"would not have\")\n","    text = text.replace(\"didn't\", \"did not\")\n","    text = text.replace(\"doesn't\", \"does not\")\n","    text = text.replace(\"don't\", \"do not\")\n","    text = text.replace(\"hadn't\", \"had not\")\n","    text = text.replace(\"hadn't've\", \"had not have\")\n","    text = text.replace(\"hasn't\", \"has not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"he'd\", \"he would\")\n","    text = text.replace(\"haven't\", \"have not\")\n","    text = text.replace(\"he'd've\", \"he would have\")\n","    text = text.replace(\"'s\", \"\")\n","    text = text.replace(\"'t\", \"\")\n","    text = text.replace(\"'ve\", \"\")\n","    text = text.replace(\".\", \" . \")\n","    text = text.replace(\"!\", \" ! \")\n","    text = text.replace(\"?\", \" ? \")\n","    text = text.replace(\";\", \" ; \")\n","    text = text.replace(\":\", \" : \")\n","    text = text.replace(\",\", \" , \")\n","    text = text.replace(\"´\", \"\")\n","    text = text.replace(\"‘\", \"\")\n","    text = text.replace(\"’\", \"\")\n","    text = text.replace(\"“\", \"\")\n","    text = text.replace(\"”\", \"\")\n","    text = text.replace(\"\\'\", \"\")\n","    text = text.replace(\"\\\"\", \"\")\n","    text = text.replace(\"-\", \"\")\n","    text = text.replace(\"–\", \"\")\n","    text = text.replace(\"—\", \"\")\n","    text = text.replace(\"[\", \"\")\n","    text = text.replace(\"]\",\"\")\n","    text = text.replace(\"{\",\"\")\n","    text = text.replace(\"}\", \"\")\n","    text = text.replace(\"/\", \"\")\n","    text = text.replace(\"|\", \"\")\n","    text = text.replace(\"(\", \"\")\n","    text = text.replace(\")\", \"\")\n","    text = text.replace(\"$\", \"\")\n","    text = text.replace(\"+\", \"\")\n","    text = text.replace(\"*\", \"\")\n","    text = text.replace(\"%\", \"\")\n","    text = text.replace(\"#\", \"\")\n","    text = text.replace(\"\\n\", \" \\n \")\n","    text = text.replace(\"\\n\", \"\")\n","    text = text.replace(\"_\", \" _ \")\n","    text = text.replace(\"_\", \"\")\n","    text = ''.join([i for i in text if not i.isdigit()])\n","\n","    return text\n","\n","positive_reviews = reviews_df['Positive_Review'].values\n","negative_reviews = reviews_df['Negative_Review'].values\n","\n","cleaned_positive_reviews = [clean(r) for r in positive_reviews] \n","cleaned_negative_reviews = [clean(r) for r in negative_reviews] \n","\n","reviews_df['Positive_Review'] = cleaned_positive_reviews\n","reviews_df['Negative_Review'] = cleaned_negative_reviews"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8Cxz4uf3U63","colab_type":"code","colab":{}},"source":["# Shuffling data\n","reviews_df = reviews_df.sample(frac=1).reset_index(drop=True)\n","\n","# Extracting all text\n","positive_reviews = reviews_df['Positive_Review'].values\n","negative_reviews = reviews_df['Negative_Review'].values\n","reviews_text = []\n","\n","for p,n in zip(positive_reviews, negative_reviews) : \n","    if p in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n","        reviews_text.append(n)\n","    elif n in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n","        reviews_text.append(p)\n","    else : \n","        reviews_text.append(n)\n","        reviews_text.append(p)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kV_NPPF3W5H","colab_type":"code","colab":{}},"source":["# Preprocessing training data\n","training_df = reviews_df.loc[:10000]\n","positive_reviews_filtered = training_df['Positive_Review'].values\n","negative_reviews_filtered = training_df['Negative_Review'].values\n","training_reviews = []\n","labels = []\n","\n","for idx,(p,n) in enumerate(zip(positive_reviews_filtered, negative_reviews_filtered)) : \n","    if p in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n","        training_reviews.append(n)\n","        labels.append(0)\n","    elif n in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] :\n","        training_reviews.append(p)\n","        labels.append(1)\n","    else :\n","        training_reviews.append(n)\n","        labels.append(0)\n","        training_reviews.append(p)\n","        labels.append(1)\n","\n","# Creating datasets\n","dict1 ={\n","    'reviews' : training_reviews,\n","    'labels' : labels\n","}\n","sentiment_df = pd.DataFrame.from_dict(dict1)\n","\n","\n","dict2 ={\n","    'reviews_text' : reviews_text\n","}\n","reviews_text_df = pd.DataFrame.from_dict(dict2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IeNxA-F3Y5V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"916034f5-0731-473e-8797-80cace46b9ff","executionInfo":{"status":"ok","timestamp":1589488289449,"user_tz":-330,"elapsed":317003,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["text_reviews = [str(r) for r in reviews_text_df['reviews_text'].values]\n","\n","sentences = []\n","\n","for review in text_reviews:\n","    words = text_to_word_sequence(review)\n","    sentences.append(words)\n","\n","embeddings_model = Word2Vec(sentences, min_count=1, sg=1, size=128)\n","words = list(embeddings_model.wv.vocab)\n","print('{} WORDS '.format(len(words)))\n","print('Printing first 100:')\n","print(words[:100])"],"execution_count":45,"outputs":[{"output_type":"stream","text":["76904 WORDS \n","Printing first 100:\n","['great', 'service', 'plush', 'rooms', 'and', 'a', 'very', 'comfortable', 'bed', 'the', 'staff', 'was', 'extremely', 'accommodating', 'my', 'travel', 'plans', 'changed', 'i', 'arrived', 'two', 'days', 'early', 'web', 'site', 'said', 'it', 'fully', 'booked', 'but', 'when', 'contacted', 'them', 'directly', 'room', 'available', 'from', 'that', 'moment', 'on', 'just', 'top', 'notch', 'first', 'in', 'old', 'english', 'manor', 'style', 'can', 't', 'say', 'enough', 'about', 'beds', 'super', 'second', 'teresa', 'suite', 'omg', 'fantastic', 'felt', 'like', 'lord', 'of', 'could', 'not', 'have', 'been', 'happier', 'with', 'choice', 'hotel', 'close', 'to', 'everything', 'small', 'overpriced', 'had', 'get', 'shower', 'gel', 'shampoo', 'reception', 'desk', 'each', 'day', 'good', 'location', 'breakfast', 'is', 'euro', 'which', 'expensive', 'last', 'air', 'conditioning', 'didn', 'work', 'properly']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pf_dwLWq3a_N","colab_type":"code","colab":{}},"source":["import keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hYg7dqvZ3c_r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"aac7f667-8be5-49d2-d040-4430c37611f3","executionInfo":{"status":"ok","timestamp":1589488308913,"user_tz":-330,"elapsed":321100,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["# Querying SQLlite database to extract needed words embeddings\n","tokenizer = keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(text_reviews)\n","vocabSize = len(tokenizer.word_index) + 1\n","vocabSize"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["76905"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"essGNTI93ev0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"5cfc5452-b5ad-44e0-d356-f67064d15a9a","executionInfo":{"status":"ok","timestamp":1589488308922,"user_tz":-330,"elapsed":313002,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["vocabSize = len(tokenizer.word_index) + 1\n","print(vocabSize)\n","\n","# Recreating embeddings index based on Tokenizer vocabulary\n","word2vec_vocabulary = embeddings_model.wv.vocab\n","embeddingIndex = dict()\n","counter = 0\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_vocabulary :\n","        embeddingIndex[word] = embeddings_model[word]\n","    else:\n","        counter += 1\n","\n","print(\"{} words without pre-trained embedding!\".format(counter))\n","    \n","# Prepare embeddings matrix\n","embeddingMatrix = zeros((vocabSize, 128))\n","for word, i in tokenizer.word_index.items():\n","    embeddingVector = embeddingIndex.get(word)\n","    if embeddingVector is not None:\n","        embeddingMatrix[i] = embeddingVector"],"execution_count":48,"outputs":[{"output_type":"stream","text":["76905\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"},{"output_type":"stream","text":["0 words without pre-trained embedding!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cIpsR1zH3guV","colab_type":"code","colab":{}},"source":["reviews = [ str(r) for r in sentiment_df['reviews'].values]\n","labels = sentiment_df['labels'].values\n","\n","for idx, review in enumerate(text_reviews):\n","    words = text_to_word_sequence(review)\n","    if(len(words) > 40): \n","        words = words[:40]\n","        text_reviews[idx] = ' '.join(words)\n","\n","oneHotReviews = tokenizer.texts_to_sequences(reviews)\n","encodedReviews = keras.preprocessing.sequence.pad_sequences(oneHotReviews, maxlen=40, padding='post')\n","x_train, x_val, y_train, y_val = train_test_split(encodedReviews, labels, test_size=0.2, random_state=42)\n","x_test, x_val, y_test, y_val = train_test_split(encodedReviews, labels, test_size=0.5, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pagi3CCu3KPG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q22w3wTlTQ1L","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import os\n","from tensorflow.python.keras.layers import Layer\n","from tensorflow.python.keras import backend as K\n","from keras.layers import Embedding\n","from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.merge import concatenate\n","\n","\n","class AttentionLayer(Layer):\n","    \"\"\"\n","    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n","    There are three sets of weights introduced W_a, U_a, and V_a\n","     \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert isinstance(input_shape, list)\n","        # Create a trainable weight variable for this layer.\n","\n","        self.W_a = self.add_weight(name='W_a',\n","                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","        self.U_a = self.add_weight(name='U_a',\n","                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","        self.V_a = self.add_weight(name='V_a',\n","                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","\n","        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, inputs, verbose=False):\n","        \"\"\"\n","        inputs: [encoder_output_sequence, decoder_output_sequence]\n","        \"\"\"\n","        assert type(inputs) == list\n","        encoder_out_seq, decoder_out_seq = inputs\n","        if verbose:\n","            print('encoder_out_seq>', encoder_out_seq.shape)\n","            print('decoder_out_seq>', decoder_out_seq.shape)\n","\n","        def energy_step(inputs, states):\n","            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n","\n","            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n","            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n","\n","            \"\"\" Some parameters required for shaping tensors\"\"\"\n","            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n","            de_hidden = inputs.shape[-1]\n","\n","            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n","            # <= batch_size*en_seq_len, latent_dim\n","            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n","            # <= batch_size*en_seq_len, latent_dim\n","            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n","            if verbose:\n","                print('wa.s>',W_a_dot_s.shape)\n","\n","            \"\"\" Computing hj.Ua \"\"\"\n","            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n","            if verbose:\n","                print('Ua.h>',U_a_dot_h.shape)\n","\n","            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n","            # <= batch_size*en_seq_len, latent_dim\n","            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n","            if verbose:\n","                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n","\n","            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n","            # <= batch_size, en_seq_len\n","            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n","            # <= batch_size, en_seq_len\n","            e_i = K.softmax(e_i)\n","\n","            if verbose:\n","                print('ei>', e_i.shape)\n","\n","            return e_i, [e_i]\n","\n","        def context_step(inputs, states):\n","            \"\"\" Step function for computing ci using ei \"\"\"\n","            # <= batch_size, hidden_size\n","            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n","            if verbose:\n","                print('ci>', c_i.shape)\n","            return c_i, [c_i]\n","\n","        def create_inital_state(inputs, hidden_size):\n","            # We are not using initial states, but need to pass something to K.rnn funciton\n","            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n","            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n","            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n","            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n","            return fake_state\n","\n","        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n","        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n","\n","       # \"\"\" Computing energy outputs \"\"\"\n","        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n","        last_out, e_outputs, _ = K.rnn(\n","            energy_step, decoder_out_seq, [fake_state_e],\n","        )\n","\n","       # \"\"\" Computing context vectors \"\"\"\n","        last_out, c_outputs, _ = K.rnn(\n","            context_step, e_outputs, [fake_state_c],\n","        )\n","\n","        return c_outputs, e_outputs\n","\n","    def compute_output_shape(self, input_shape):\n","      #  \"\"\" Outputs produced by the layer \"\"\"\n","        return [\n","            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n","            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n","        ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Gv5t_Nl67Bi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":615},"outputId":"a2b4453c-2574-4af4-dc3a-bac545d089c2","executionInfo":{"status":"ok","timestamp":1589488919163,"user_tz":-330,"elapsed":5609,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}}},"source":["!pip install tensorflow\n"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.0.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.28.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (46.1.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.7.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.2.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Z_2CeR5-NEK","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import re\n","import pickle\n","from collections import defaultdict\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_curve,auc,confusion_matrix\n","from bs4 import BeautifulSoup\n","\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","\n","from keras.layers import Embedding, Dense, Input, Flatten, Concatenate\n","from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM, GRU, Bidirectional, TimeDistributed \n","from keras.models import Model, load_model\n","\n","from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import initializers\n","from keras.preprocessing import sequence\n","from keras.callbacks import ModelCheckpoint\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras.utils import plot_model\n","\n","from nltk import tokenize\n","import matplotlib.pyplot as plt\n","iteration1 = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0URP9JJw-N8U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Rty1D4469UH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iv_HCWFiqyv3","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import Flatten\n","from keras.initializers import Constant"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXrVoN7ZqzkN","colab_type":"code","colab":{}},"source":["from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.merge import concatenate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XyuOqt37U0M6","colab_type":"code","outputId":"69e6bad0-f7d6-4b06-9ded-1119c802ce78","executionInfo":{"status":"error","timestamp":1589488782584,"user_tz":-330,"elapsed":1874,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["MAX_WORD_NUM=200\n","embedding_layer = Embedding(len(word_index) + 1,EMBED_SIZE,weights=[embedding_matrix], \n","                            input_length=MAX_WORD_NUM, trainable=False,name='word_embedding')\n","# Words level attention model\n","word_input = Input(shape=(MAX_WORD_NUM,), dtype='int32',name='word_input')\n","word_sequences = embedding_layer(word_input)\n","word_gru = Bidirectional(GRU(50, return_sequences=True),name='word_gru')(word_sequences)\n","word_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \n","word_att,word_coeffs = AttentionLayer(EMBED_SIZE,True,name='word_attention')(word_dense)\n","wordEncoder = Model(inputs = word_input,outputs = word_att)\n","\n","# Sentence level attention model\n","sent_input = Input(shape=(MAX_SENTENCE_NUM,MAX_WORD_NUM), dtype='int32',name='sent_input')\n","sent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\n","sent_gru = Bidirectional(GRU(50, return_sequences=True),name='sent_gru')(sent_encoder)\n","sent_dense = Dense(100, activation='relu', name='sent_dense')(sent_gru) \n","sent_att,sent_coeffs = AttentionLayer(EMBED_SIZE,return_coefficients=True,name='sent_attention')(sent_dense)\n","sent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\n","preds = Dense(5, activation='softmax',name='output')(sent_drop)\n","\n","# Model compile\n","model = Model(sent_input, preds)\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n","\n","print(wordEncoder.summary())\n","print(model.summary())"],"execution_count":55,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-67e84764fab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                             input_length=MAX_WORD_NUM, trainable=False,name='word_embedding')\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#encoder_inputs = Input(shape=(None, num_encoder_tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Words level attention model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mword_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_WORD_NUM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word_input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Bidirectional' is not defined"]}]},{"cell_type":"code","metadata":{"id":"wrlvaz4ao6fc","colab_type":"code","outputId":"ff1ac122-12de-4f8f-e389-7223c2804b3e","executionInfo":{"status":"error","timestamp":1589488849334,"user_tz":-330,"elapsed":2005,"user":{"displayName":"Shubham Kumar","photoUrl":"","userId":"13299555443270476721"}},"colab":{"base_uri":"https://localhost:8080/","height":663}},"source":["model = tf.keras.Sequential()\n","history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=7, batch_size=32)"],"execution_count":58,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-0b0af94921d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/version_utils.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.engine.base_layer_v1'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"BnE29KN9sImt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYAPNjxwm8BP","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","\n","import numpy as np\n","from keras import backend as K\n","from keras.layers import Layer\n","from keras import initializers, regularizers, constraints\n","\n","\n","def _softmax(x, dim):\n","    \"\"\"Computes softmax along a specified dim. Keras currently lacks this feature.\n","    \"\"\"\n","\n","    if K.backend() == 'tensorflow':\n","        import tensorflow as tf\n","        return tf.nn.softmax(x, dim)\n","    elif K.backend() is 'cntk':\n","        import cntk\n","        return cntk.softmax(x, dim)\n","    elif K.backend() == 'theano':\n","        # Theano cannot softmax along an arbitrary dim.\n","        # So, we will shuffle `dim` to -1 and un-shuffle after softmax.\n","        perm = np.arange(K.ndim(x))\n","        perm[dim], perm[-1] = perm[-1], perm[dim]\n","        x_perm = K.permute_dimensions(x, perm)\n","        output = K.softmax(x_perm)\n","\n","        # Permute back\n","        perm[dim], perm[-1] = perm[-1], perm[dim]\n","        output = K.permute_dimensions(x, output)\n","        return output\n","    else:\n","        raise ValueError(\"Backend '{}' not supported\".format(K.backend()))\n","\n","\n","class AttentionLayer(Layer):\n","    \"\"\"Attention layer that computes a learned attention over input sequence.\n","    For details, see papers:\n","    - https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\n","    - http://colinraffel.com/publications/iclr2016feed.pdf (fig 1)\n","    Input:\n","        x: Input tensor of shape `(..., time_steps, features)` where `features` must be static (known).\n","    Output:\n","        2D tensor of shape `(..., features)`. i.e., `time_steps` axis is attended over and reduced.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 kernel_initializer='he_normal',\n","                 kernel_regularizer=None,\n","                 kernel_constraint=None,\n","                 use_bias=True,\n","                 bias_initializer='zeros',\n","                 bias_regularizer=None,\n","                 bias_constraint=None,\n","                 use_context=True,\n","                 context_initializer='he_normal',\n","                 context_regularizer=None,\n","                 context_constraint=None,\n","                 attention_dims=None,\n","                 **kwargs):\n","        \"\"\"\n","        Args:\n","            attention_dims: The dimensionality of the inner attention calculating neural network.\n","                For input `(32, 10, 300)`, with `attention_dims` of 100, the output is `(32, 10, 100)`.\n","                i.e., the attended words are 100 dimensional. This is then collapsed via summation to\n","                `(32, 10, 1)` to indicate the attention weights for 10 words.\n","                If set to None, `features` dims are used as `attention_dims`. (Default value: None)\n","        \"\"\"\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","\n","        super(AttentionLayer, self).__init__(**kwargs)\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","\n","        self.use_bias = use_bias\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","\n","        self.use_context = use_context\n","        self.context_initializer = initializers.get(context_initializer)\n","        self.context_regularizer = regularizers.get(context_regularizer)\n","        self.context_constraint = constraints.get(context_constraint)\n","\n","        self.attention_dims = attention_dims\n","        self.supports_masking = True\n","\n","    def build(self, input_shape):\n","        if len(input_shape) < 3:\n","            raise ValueError(\"Expected input shape of `(..., time_steps, features)`, found `{}`\".format(input_shape))\n","\n","        attention_dims = input_shape[-1] if self.attention_dims is None else self.attention_dims\n","        self.kernel = self.add_weight(shape=(input_shape[-1], attention_dims),\n","                                      initializer=self.kernel_initializer,\n","                                      name='kernel',\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","\n","        if self.use_bias:\n","            self.bias = self.add_weight(shape=(attention_dims, ),\n","                                        initializer=self.bias_initializer,\n","                                        name='bias',\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","        else:\n","            self.bias = None\n","\n","        if self.use_context:\n","            self.context_kernel = self.add_weight(shape=(attention_dims, ),\n","                                                  initializer=self.context_initializer,\n","                                                  name='context_kernel',\n","                                                  regularizer=self.context_regularizer,\n","                                                  constraint=self.context_constraint)\n","        else:\n","            self.context_kernel = None\n","\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, x, mask=None):\n","        # x: [..., time_steps, features]\n","        # ut = [..., time_steps, attention_dims]\n","        ut = K.dot(x, self.kernel)\n","        if self.use_bias:\n","            ut = K.bias_add(ut, self.bias)\n","\n","        ut = K.tanh(ut)\n","        if self.use_context:\n","            ut = ut * self.context_kernel\n","\n","        # Collapse `attention_dims` to 1. This indicates the weight for each time_step.\n","        ut = K.sum(ut, axis=-1, keepdims=True)\n","\n","        # Convert those weights into a distribution but along time axis.\n","        # i.e., sum of alphas along `time_steps` axis should be 1.\n","        self.at = _softmax(ut, dim=1)\n","        if mask is not None:\n","            self.at *= K.cast(K.expand_dims(mask, -1), K.floatx())\n","\n","        # Weighted sum along `time_steps` axis.\n","        return K.sum(x * self.at, axis=-2)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]\n","\n","    def get_attention_tensor(self):\n","        if not hasattr(self, 'at'):\n","            raise ValueError('Attention tensor is available after calling this layer with an input')\n","        return self.at\n","\n","    def get_config(self):\n","        config = {\n","            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n","            'bias_initializer': initializers.serialize(self.bias_initializer),\n","            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n","            'bias_constraint': constraints.serialize(self.bias_constraint),\n","            'context_initializer': initializers.serialize(self.context_initializer),\n","            'context_regularizer': regularizers.serialize(self.context_regularizer),\n","            'context_constraint': constraints.serialize(self.context_constraint)\n","        }\n","        base_config = super(AttentionLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class ConsumeMask(Layer):\n","    \"\"\"Layer that prevents mask propagation.\n","    \"\"\"\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def call(self, x, mask=None):\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibK47a4hsARg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXdrjESVVXe-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kkSQ6fmNKDC-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkeApgYl-dqC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}